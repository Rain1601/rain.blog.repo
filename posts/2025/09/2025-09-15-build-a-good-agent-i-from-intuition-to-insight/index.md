---
title: "Build A Good Agent I. From intuition to insight"
date: "2025-09-15"
tags: []
categories: []
author: "Muses"
summary: ""
---

<h1>导读</h1><p>在上篇Build A Good Agent Part1: From intuition to insight 一文中，我们在Part1-4中完整介绍了Agent的定义，所需的能力，设计的范式，以及人如何与Agent的交互四个部分。我们更多是加深对于Agent的理解，可以判断什么场景下是需要Agent的，并且也理解如何将我们的场景设计成一个好的Agent产品。</p><p>Anthropic指出从业界实践看，成功的案例不来自于复杂专业的框架，而是使用了简单可组合的方式</p><blockquote><p>Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.</p></blockquote><p>而</p><h1>Part1. Agent的实现</h1><p>对于Agent的实现，无论是大模型厂家还是一些团队都给出了自己的Agent产品。OpenAI给出了，Google给出了用于xxx的Gemini CLI，Anthropic做出了Claude Code。以及微软的AutoGPT。并且还有很多Agent的构建框架如Langraph，CrewAI。但是根据实际的实践过程，发现从零开始一点一点构建自己的Agent相比上来就用各种各样的框架，使用Cursor或者Claude Code帮自己实现而言，是可以更好的实现一个理想的Agent的）。Anthropic在其文章中<a target="_blank" rel="noopener noreferrer nofollow" href="https://www.anthropic.com/engineering/building-effective-agents">Building Effective Agents</a>的总结部分写道了实现Agent的前提和三个核心原则：</p><blockquote><p>在大语言模型（LLM）领域取得成功，并不是要构建最复杂的系统，而是要构建<strong>最适合自身需求的系统</strong>。从简单的提示词开始，通过全面的评估不断优化；只有在简单方案不足以满足需求时，再引入多步骤的智能体系统。</p></blockquote><p>在实现智能体时，我们遵循三大核心原则：</p><ol><li><p><strong>保持设计的简洁性</strong> —— 尽量减少不必要的复杂度。</p></li><li><p><strong>优先保证透明性</strong> —— 通过明确展示智能体的规划步骤，让用户理解其决策逻辑。</p></li><li><p><strong>精心设计智能体-计算机接口（agent-computer interface, ACI）</strong> —— 借助完善的工具文档与充分的测试，确保接口稳健可靠。</p></li></ol><p>各类框架能帮助你快速上手，但在进入生产环境时，不要犹豫去<strong>减少抽象层</strong>，直接使用基础组件来搭建。遵循这些原则，你就能打造出既强大、又可靠、可维护并且能获得用户信任的智能体。</p><p>在Software2.0的时代中，我们最常使用也最常接触的是应用程序接口（application program interface, API）和JSON，API 定义了通信的“接口”和软件中介之间的交互，而JSON 规定了传输“数据”的格式，而在LLM时代下，定义智能体和系统之间的交互变得尤为重要，特别是在于系统对于接口需要JSON。这也是为什么</p><p>在我们动手实现了自己的Agent后，再考虑看看Longraph的框架会更有收获和裨益。在本章中，这几种方式都会进行实践，然后从个人开发感受，最终实现效果，后期维护成本上来总结。</p><p>在</p><h2>1.1 基于SDK实现（古法手搓）</h2><p></p>![](./images/image-1.svg)<p></p>![](./images/image-2.svg)<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf">https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf</a></p><h2>1.2 基于框架实现</h2><p>创建Agent的框架有很多很多，那么这些框架都做了什么呢？如何看出一个框架的好坏。引入框架后是增加了复杂度还是降低了复杂度呢？</p><p>综合考虑后最终选择了Langraph，</p><h2>1.3实现对比</h2><h1>Part2. 让Agent更聪明</h1><p>我绘制了一份自己理解下基于一次调用Single-Agent中所参与的模块和流程，</p><p></p>![](./images/image-3.svg)<ul><li><p>[ ] Prompt里面需要添加动态的部分</p></li></ul><p>从上面的组成我们不难看出Agent的核心在于Prompt，Context，Tool和Model，</p><h2>2.1 模型和思考</h2><h3>模型的选择</h3><p>这一步绝对不是水字数，而是应证了什么叫大道至简，Model作为Agent的大脑，有时候各种手段层出不穷的效果不如一个升级基模。</p><p>曾经的我：要是我有学霸的脑子就好了：</p><p></p>![](./images/image-4.png)<p>现在的我：Claude Sonset4解决不了，试试Opus4，Opus4解决不了，试试Opus4.1，Opus4.1解决不了，那凭我的脑子也解决不了。</p><p></p>![](./images/image-5.jpeg)<p>当然，这种方法虽然行之有效，但我们也不能指望今天推出Claude4，明天发布Claude5，后天更新Claude6……而且不知道大家有没有感受OpenAI的GPT5代已经没有那么惊艳的效果了</p><p>并且从Claude和OpenAI的迭代思路看，它们对外的模型更偏向综合，至少目前没有对外提供垂直领域的专业模型（但是我相信它们已经做出了很多内部产品专供企业级客户）</p><p>在选择模型上，OpenAI在其文章A practical  guide to  building agents的Selecting your models章节提到一个有效准则：</p><blockquote><p>首先，为所有任务都使用能力最强的模型来构建你的代理原型，以建立一个性能基线。在此基础上，再尝试替换使用更小的模型，看看它们是否仍能达到可接受的结果。这样，既不会过早地限制代理的能力，也能诊断出小模型在哪些地方成功或失败。</p></blockquote><p>总而言之，选择模型的原则很简单：</p><ul><li><p><strong>01</strong> 建立评估体系以设定性能基线。</p></li><li><p><strong>02</strong> 专注于使用可用的最佳模型来达到你的准确性目标。</p></li><li><p><strong>03</strong> 在可能的情况下，用更小的模型替换更大的模型，以优化成本和延迟。</p></li></ul><p>截止本文写作的时间2025年9月，目前主流的模型有OpenAI-GPT5，Anthropic-Claude4Sonnet，Google-Gemini2.5pro。我喜欢使用在我希望要去做的业务领域里面，自己曾经思考过的最具有难度和最泛化常用的问题，向这些模型发起挑战，根据它们的回复来判断要选择哪一个模型。基于测试，OpenAI的模型回复更加乐观，而Claude模型更加严谨，Gemini则是介于二者之间。</p><h3>模型的思考方式</h3><p>针对具体问题，模型可以依靠选择不同的思考方式来达到更好的效果。</p><h3>模型的工程接入</h3><p>在项目的初始化阶段，</p><p>除非个人对某个模型非常的情有独钟，还是建议在工程实现初期就做好多模型的接入并且为不同的模型配备好不同的SDK，特别是不同厂家会提供不同的SDK</p><p></p>![](./images/image-6.png)<p></p>![](./images/image-7.png)<h2>2.2 上下文工程</h2><p></p>![](./images/image-8.png)<p></p>![](./images/image-9.png)<h3>上下文和Prompt</h3><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md">https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md</a></p><p></p>![](./images/image-10.png)<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d">https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d</a></p><p></p>![](./images/image-11.png)<p>在实践上下文的过程中我发现这个词最早在LLM官方的Blog中指出并描述是在的这边文章：<a target="_blank" rel="noopener noreferrer nofollow" href="https://www.anthropic.com/engineering/contextual-retrieval">https://www.anthropic.com/engineering/contextual-retrieval</a>。</p><p>它提出了：</p><p>而这个词从提出到走向火热到进入企业视野是在xxx号Kaparthy的演讲后。</p><p>更清晰的上下文</p><p>Context Engineering</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://manus.im/zh-cn/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">https://manus.im/zh-cn/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/davidkimai/Context-Engineering">https://github.com/davidkimai/Context-Engineering</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/coleam00/context-engineering-intro">https://github.com/coleam00/context-engineering-intro</a></p><p>任务执行的信息</p><p>Spring Context</p><p>系统化的环境</p><p><strong>“与其 prompt 更好，不如 context 更全。”</strong></p><p>——构建一个<strong>系统化的上下文</strong>环境，让 AI 像人类工程师一样在约束中产出可维护、可验证、可迭代的代码。</p><p><strong>Context Engineering ≠ Prompt Engineering</strong></p><p>它不是一句句 prompt 地调 AI，而是通过结构化文件、项目规范、样例代码、工作流程等“上下文构件”构建一整套工程环境，让 AI 像“加入你的项目”的同事一样工作。</p><p>Prompt 是命令，而 Context 是环境。针对Task或者最终的Final Goal，上下文的作用不言而喻。</p><p><strong>构建 AI 的“思维工作内存”</strong></p><ul><li><p>根据Kaparthy在xxx的演讲中所说，针对Software3.0为代表的Claude、GPT 等模型的 context window 就像 RAM。</p></li><li><p>开发者的职责，不再是“指令员”，而是“上下文工程师”——把有用的信息精准、结构化地放入上下文中：</p><ul><li><p>项目规范（<a target="_blank" rel="noopener noreferrer nofollow" href="http://CLAUDE.md">CLAUDE.md</a>）</p></li><li><p>代码样例（examples/）</p></li><li><p>任务描述（<a target="_blank" rel="noopener noreferrer nofollow" href="http://INITIAL.md">INITIAL.md</a>）</p></li><li><p>需求蓝图（PRP）</p></li><li><p>验证机制（自动测试）</p></li></ul></li></ul><p>3.<strong>AI 需要“软规则”而不是“硬指令”</strong></p><ul><li><p>比如不是写：“用 Python 实现 XX”，而是：</p><ul><li><p>给它看同项目已有 Python 代码风格</p></li><li><p>给它写的测试规范</p></li><li><p>给它 API 使用方式示例</p></li></ul></li><li><p>AI 会在理解这些“风格/偏好/限制”的基础上，自然模仿和生成正确内容</p></li></ul><p>4.<strong>功能开发要结构化，不靠拍脑袋 prompt</strong></p><ul><li><p>开发一个功能的流程被拆解为：</p><ul><li><p><code>INITIAL.md</code>（初始需求）</p></li><li><p><code>/generate-prp</code>：让 Claude 生成 Product Requirement Prompt（PRP）</p></li><li><p><code>/execute-prp</code>：让 Claude 写代码并执行测试</p></li><li><p>自动化测试验证后写入 PRPs 归档</p></li></ul></li></ul><p>这是一种全新的<strong>代码协作范式</strong>：</p><blockquote><p>“不是让 AI 写代码，而是让 AI 在我们给的工程上下文中承担代码工程师的角色。”</p></blockquote><p>上下文工程意图解决什么问题？</p><table style="min-width: 50px;"><colgroup><col style="min-width: 25px;"><col style="min-width: 25px;"></colgroup><tbody><tr><th colspan="1" rowspan="1"><p>传统 Prompt 工程</p></th><th colspan="1" rowspan="1"><p>Context 工程</p></th></tr><tr><td colspan="1" rowspan="1"><p>每次都要重新解释规范</p></td><td colspan="1" rowspan="1"><p>一次定义，全局引用</p></td></tr><tr><td colspan="1" rowspan="1"><p>输出不稳定、风格混乱</p></td><td colspan="1" rowspan="1"><p>模仿样例 + 风格指导</p></td></tr><tr><td colspan="1" rowspan="1"><p>AI 产物难测试、难维护</p></td><td colspan="1" rowspan="1"><p>自带测试要求和验证流程</p></td></tr><tr><td colspan="1" rowspan="1"><p>交互是单轮 prompt</p></td><td colspan="1" rowspan="1"><p>是一个<strong>多轮、具备 memory 的流程</strong></p></td></tr><tr><td colspan="1" rowspan="1"><p>Prompt 是手艺活儿</p></td><td colspan="1" rowspan="1"><p>Context 是系统工程</p></td></tr></tbody></table><blockquote><p><strong>Prompt engineering 是工匠打磨产品，Context engineering 是工程师搭建框架。</strong></p></blockquote><p>你把 Claude 或 GPT 培养成一个“工程师”，不是用 prompt 去控制它，而是构建一个「结构化上下文环境」——它读懂你的规则、规范、任务、代码风格，然后主动输出高质量结果。</p><ul><li><p>上下文和Prompt的区别是什么？</p></li><li><p>上下文有哪些类别？</p></li><li><p>为什么Claude Code 要压缩上下文？</p></li></ul><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://medium.com/@vipra_singh/ai-agents-agentic-memory-part-9-abb3fdbe486c">https://medium.com/@vipra_singh/ai-agents-agentic-memory-part-9-abb3fdbe486c</a></p><h3>上下文构建</h3><p>JSON vs自然语言</p><p>模版支持动态注入</p><h3>上下文压缩</h3><p></p>![](./images/image-12.png)<p>通过上图我们可以看到，Agent在执行过程中是一个Loop执行的过程，每一次循环的执行结果会注入到Context模块中，理论上Agent持续执行，Context可以无限叠加，想法虽然美好，但是LLM对于请求的大小是有上限的，以Claude4为例，其API对于上下文窗口的大小限制为200Ktoken。</p><p>但是这也告诉我们，如果上下文不足200Ktoken甚至不足100ktoken，对于其的压缩，优化都是没有必要的。</p><p>Claude Code: 上下文工程 system-reminder</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://mp.weixin.qq.com/s/lBhZhdlb1s0y4qgl_5HSWQ">https://mp.weixin.qq.com/s/lBhZhdlb1s0y4qgl_5HSWQ</a></p><p></p>![](./images/image-13.png)<h2>2.3 提示词工程</h2><p>"Code is cheap, show me your prompt."</p><p></p>![](./images/image-14.png)<p>大家是否留意到，每当厂商发布新模型时往往会同步更新该模型的 Prompt 指南或实践文档。</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices">https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide">https://cookbook.openai.com/examples/gpt4-1_prompting_guide</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide">https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide</a></p><p>模型是大脑，而大脑可以用来思考很多问题，而和大模型直接交互的就是已Prompt的方式来实现</p><p></p>![](./images/image-15.png)<p>再好的脑子，没有了好的引导和指令也是缺少生产力的，一个好的Prompt。</p><p>Prompt的核心作用</p><p>模型</p><p>对于脑子的使用人与人就天差地别了，这也是我认为我们最能最能去研究的地方，Prompt Engineering是新的能力，而Prompt是新的资产。从"Do not code, show me the talk"这句slogan可以看出，Prompting是取代Coding的新一代能力，它更加符合直觉，所以也侧面要求对我们自身对于问题的定义能力。Prompt 是xx， Prompt 是函数，Prompt也是思维的定义（GPT4.1发布时说明支持了Prompt），如何基于真实的业务场景写出一个好的Prompt呢？</p><ul><li><p>Prompt是如何影响LLM的呢？</p></li><li><p>认知-身份-目标</p></li></ul><p>最佳阅读材料</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview">https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/anthropics/courses">https://github.com/anthropics/courses</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/anthropics/prompt-eng-interactive-tutorial">https://github.com/anthropics/prompt-eng-interactive-tutorial</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.promptingguide.ai/zh">https://www.promptingguide.ai/zh</a></p><h3>提示词的范式</h3><h3>提示词的功能</h3><h4>控制输出JSON格式</h4><p>预填充 Claude 的回复以获得更好的输出控制</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://docs.anthropic.com/zh-CN/docs/build-with-claude/prompt-engineering/prefill-claudes-response">https://docs.anthropic.com/zh-CN/docs/build-with-claude/prompt-engineering/prefill-claudes-response</a></p><p>提高输出一致性（JSON模式）</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://docs.anthropic.com/zh-CN/docs/test-and-evaluate/strengthen-guardrails/increase-consistency">https://docs.anthropic.com/zh-CN/docs/test-and-evaluate/strengthen-guardrails/increase-consistency</a></p><pre><code class="language-Python"># 创建消息列表，使用Assistant预填充确保JSON响应
messages = [
    LLMMessage(role="system", content=system_prompt),
    LLMMessage(role="user", content=user_prompt),
    LLMMessage(role="assistant", content="{")  # 预填充确保JSON格式
]
</code></pre><p>注：这里提供都是对于Agent实现的例子</p><h4>实现网络搜索</h4><p>开发者文档：</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://docs.anthropic.com/zh-CN/docs/agents-and-tools/tool-use/web-search-tool">https://docs.anthropic.com/zh-CN/docs/agents-and-tools/tool-use/web-search-tool</a></p><p>功能介绍文档：<a target="_blank" rel="noopener noreferrer nofollow" href="https://www.anthropic.com/news/web-search-api">https://www.anthropic.com/news/web-search-api</a></p><h3>提示词是如何影响模型回复的？</h3><h3>提示词的工程接入</h3><blockquote><p>Prompt的层级管理 Prompt的版本管理</p></blockquote><p>Gemini CLI 使用了一个分层的 Prompt 管理系统，允许从系统级别到项目级别，再到动态扩展的多层次配置和自定义。</p><h4>Prompt 三层结构</h4><pre><code class="language-Markdown">┌─────────────────────────────────────────┐
│         第三层：MCP Prompts             │
│    (动态注册的外部 prompt 模板)          │
├─────────────────────────────────────────┤
│       第二层：项目上下文                 │
│         (GEMINI.md 文件)                │
├─────────────────────────────────────────┤
│       第一层：核心系统 Prompt           │
│    (系统基础行为和规则定义)              │
└─────────────────────────────────────────┘
</code></pre><h4>Prompt文件组织目录</h4><pre><code class="language-Markdown">packages/core/src/
├── core/
│   └── prompts.ts           # 核心系统 prompt 定义
├── prompts/
│   ├── prompt-registry.ts   # MCP prompt 注册表
│   └── mcp-prompts.ts      # MCP prompt 工具函数
└── tools/
    └── mcp-client.ts       # MCP 客户端实现
</code></pre><h4>Prompt获取方法</h4><ul><li><p><strong>getCoreSystemPrompt(userMemory?: string)</strong>: 生成核心系统指令</p></li><li><p><strong>getCompressionPrompt()</strong>: 生成历史压缩专用 prompt</p></li></ul><p>其中<strong>getCompressionPrompt</strong>是当窗口占比达到70%时，提取历史对话压缩至30%输出为XML格式的文件保存到内存之中。而<strong>getCoreSystemPrompt</strong>是核心的获取Prompt方法，</p><pre><code class="language-Mermaid">graph TD
    Start[开始构建 Prompt] --&gt; LoadSystem[加载系统 Prompt]
    
    LoadSystem --&gt; CheckOverride{检查 system.md}
    CheckOverride --&gt;|存在| UseCustom[使用自定义]
    CheckOverride --&gt;|不存在| UseDefault[使用默认]
    
    UseCustom --&gt; AddEnv[注入环境信息]
    UseDefault --&gt; AddEnv
    
    AddEnv --&gt; DiscoverContext[发现项目上下文]
    
    DiscoverContext --&gt; SearchGemini[搜索 GEMINI.md]
    SearchGemini --&gt; ProcessImports[处理 @import]
    ProcessImports --&gt; LoadGlobal[加载全局记忆]
    
    LoadGlobal --&gt; CombineMemory[合并记忆内容]
    
    CombineMemory --&gt; CheckMCP{有 MCP 服务器?}
    CheckMCP --&gt;|是| LoadMCPPrompts[加载 MCP Prompts]
    CheckMCP --&gt;|否| SkipMCP[跳过 MCP]
    
    LoadMCPPrompts --&gt; InjectMCP[注入 MCP 上下文]
    SkipMCP --&gt; FinalAssembly[最终组装]
    InjectMCP --&gt; FinalAssembly
    
    FinalAssembly --&gt; Output[输出完整 Prompt]
    
    style Start fill:#e1f5fe
    style Output fill:#e8f5e9
</code></pre><p>MCP Prompts的注入机制</p><pre><code class="language-Mermaid">sequenceDiagram
    participant MCP as MCP Server
    participant Client as MCP Client
    participant Registry as PromptRegistry
    participant Core as Core System
    
    MCP-&gt;&gt;Client: 启动时连接
    Client-&gt;&gt;MCP: 请求可用 prompts
    MCP--&gt;&gt;Client: 返回 prompt 列表
    
    loop 每个 prompt
        Client-&gt;&gt;Registry: registerPrompt(prompt)
        Registry-&gt;&gt;Registry: 检查命名冲突
        Registry-&gt;&gt;Registry: 存储 prompt
    end
    
    Core-&gt;&gt;Registry: getAllPrompts()
    Registry--&gt;&gt;Core: 返回所有 prompts
    Core-&gt;&gt;Core: 注入到对话上下文
</code></pre><h4>Prompt版本管理</h4><h2>2.4 工具调用</h2><p>工具善其事，必先利其器。</p><p>当前业界普遍将 Tool Use 视为 Agent 的核心能力，Function Call 是其主要操作接口，并通过 MCP 协议实现模块间的高效协作。</p><h3>工具的设计</h3><p>学霸两支笔，差生文具多</p><p></p>![](./images/image-16.png)<p>实践，看<a target="_blank" rel="noopener noreferrer nofollow" href="https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview">https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview</a> 就够了</p><h3>MCP协议</h3><p>定义</p><p>MCP某种程度上可以理解为Agent使用工具的能力，那么能使用的工具越多是不是就说明这个Agent 的能力越强呢？</p><h3>工具的工程接入</h3><h2>2.5 记忆工程</h2><p>跨越会话的能力</p><p></p>![](./images/image-17.png)<p></p>![](./images/image-18.png)<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://mp.weixin.qq.com/s/SAHwfycPMZ5cU9PIXKKlow">https://mp.weixin.qq.com/s/SAHwfycPMZ5cU9PIXKKlow</a></p><p>更好的记忆设计，这一层比起Prompt来说，对于技术能力有更高的要求，但是确实非常有效的能力板块。通过实验我得出了xxx结论。 对于这里我这里先抛出对“记忆”能力产生过的一些疑问：</p><ul><li><p>为什么需要记忆工程？</p></li><li><p>什么数据可以作为记忆？思考还是结果？</p></li><li><p>用什么格式保存？JSON结构化的还是文字描述？</p></li><li><p>用什么方式保存？关系型数据库还是向量型数据库？</p></li><li><p>什么时候进行召回？在任务执行开始前，还是过程中？</p></li><li><p>对于不同的目标，记忆是否要区分？有通用记忆、领域记忆？</p></li><li><p>对于共同的目标，短期记忆、长期记忆如何去定义？</p></li><li><p>如何确保记忆的正确性？</p></li></ul><p>对<a target="_blank" rel="noopener noreferrer nofollow" href="https://mem0.ai/blog/memory-in-agents-what-why-and-how/">https://mem0.ai/blog/memory-in-agents-what-why-and-how/</a></p><h3>记忆的工程接入</h3><h2>2.6 反馈机制</h2><p>实时反馈</p><p>强化学习从推出至今，</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2303.11366">https://arxiv.org/abs/2303.11366</a></p><h2>2.7 状态和控制</h2><p>无论是对于Single-Agent还是未来将要提到的Multi-Agent，对外展示的始终是Agent所执行的Task</p><h1>Part3. Agent的评测</h1><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.anthropic.com/engineering/multi-agent-research-system">https://www.anthropic.com/engineering/multi-agent-research-system</a></p><h2>从小样本立刻开始评估</h2><p>在代理开发的早期阶段，改动往往会带来<strong>巨大的影响</strong>，因为存在大量“低垂的果实”。</p><p>一次提示词的微调就可能让成功率从<strong>30% 提升到 80%</strong>。</p><p>在这种影响幅度下，只需要很少的测试案例就能观察到明显差异。</p><p>我们一开始就建立了大约<strong>20 个查询样本</strong>，代表了真实的使用场景。测试这些查询常常能清晰展示改动的效果。</p><p>我们常听到一些 AI 开发团队推迟构建评估，理由是认为只有包含数百个测试案例的大规模评估才有价值。 但实际上，<strong>立即用少量案例开展小规模测试，比等待更全面的评估更好</strong>。</p><h2>使用 LLM 作为评判的评估（LLM-as-judge）</h2><p>当执行得当时，LLM 作为评判者的评估可以轻松扩展。</p><p>研究输出通常是自由文本，且很少有唯一正确答案，因此用程序化方法评估很困难。 <strong>LLM 天生适合对输出进行评分</strong>。</p><p>我们使用了一个 LLM 评判器，根据评分标准对每个输出进行评价，标准包括：</p><ul><li><p><strong>事实准确性</strong>：陈述是否与来源一致？</p></li><li><p><strong>引用准确性</strong>：引用的来源是否与陈述匹配？</p></li><li><p><strong>完整性</strong>：是否涵盖了所有要求的方面？</p></li><li><p><strong>来源质量</strong>：是否优先使用了高质量的原始来源，而非低质量二手来源？</p></li><li><p><strong>工具效率</strong>：是否合理使用了正确工具，次数是否合适？</p></li></ul><p>我们尝试过让多个评判者分别评估各个组件，但发现<strong>单次 LLM 调用、单一提示输出 0.0–1.0 分以及通过/不通过等级</strong> 的方法最稳定，并且最符合人工判断。</p><h2>多轮状态变更代理的最终状态评估</h2><p>对于在多轮对话中修改持久状态的代理，其评估具有<strong>独特挑战</strong>。</p><ul><li><p>与只读的研究任务不同，每一次操作都可能改变后续步骤的环境，产生依赖关系，而传统评估方法难以应对。</p></li></ul><p>我们发现，<strong>关注最终状态的评估</strong>比逐轮分析更有效。</p><ul><li><p>不必判断代理是否严格遵循了某个过程</p></li><li><p>而是评估它是否达到了<strong>正确的最终状态</strong></p></li></ul><p>这种方法承认：代理可能找到<strong>不同路径</strong> 达成相同目标，但仍能确保输出符合预期。</p><p>对于复杂工作流，可以将评估拆分为<strong>离散的检查点</strong>，检查关键状态变更是否发生，而无需验证每一个中间步骤。</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://cookbook.openai.com/examples/agents_sdk/evaluate_agents">https://cookbook.openai.com/examples/agents_sdk/evaluate_agents</a></p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/langfuse/langfuse">https://github.com/langfuse/langfuse</a></p><h1>挖坑内容</h1><p>从Single-Agent到Multi- Agent</p><h1>参考资料&amp;推荐阅读</h1><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://mp.weixin.qq.com/s/lKwk0Kb-TWhZHs8_offuvA">上下文为王：AI Agent架构的四大范式深度赏析与工程选型指南</a></p><h2>上下文相关</h2><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md">https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md</a></p><p></p>![](./images/image-19.png)<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d">https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d</a></p><p></p>![](./images/image-20.png)<p></p>